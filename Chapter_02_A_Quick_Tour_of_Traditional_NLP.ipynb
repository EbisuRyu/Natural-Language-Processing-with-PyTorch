{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👋 Xin chào mọi người! Mình là Long, và đây là bài viết thứ hai trong loạt bài chia sẻ những kiến thức mà mình đã học được từ cuốn sách *Natural Language Processing with PyTorch*. Hy vọng những chia sẻ này sẽ mang lại giá trị hữu ích cho các bạn. Hôm nay, chúng ta sẽ cùng khám phá chương tiếp theo: **Chapter 2: A Quick Tour of Traditional NLP**. 📚  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong **Computational Study of Human Language**, có hai lĩnh vực chính mà chúng ta cần lưu ý:\n",
    "\n",
    "- **Natural Language Processing (NLP)** 🧠: Mục tiêu của NLP là phát triển các phương pháp giải quyết những vấn đề thực tiễn liên quan đến ngôn ngữ, bao gồm: **information extraction** 📄, **automatic speech recognition** 🎤, **machine translation** 🌐, **sentiment analysis** 💬, **question answering** ❓, và **summarization** ✂️. NLP hướng đến việc mang lại những giải pháp thực tiễn trong thế giới số hóa hiện đại.  \n",
    "- **Computational Linguistics (CL)** 🔍: CL tập trung vào việc phát triển các phương pháp tính toán để khám phá và hiểu rõ hơn về các đặc điểm và cấu trúc của ngôn ngữ con người.\n",
    "\n",
    "Thông thường, hai lĩnh vực này có sự giao thoa mạnh mẽ, khi các phương pháp từ CL được áp dụng vào NLP và ngược lại. Tuy nhiên, trong cuốn sách này, tác giả sẽ tập trung chủ yếu vào **NLP**, đồng thời sẽ mượn một số ý tưởng từ **CL** khi cần thiết. Nội dung chính của sách là khám phá các **neural network methods** 🤖 trong NLP, đồng thời nhìn lại những khái niệm và phương pháp truyền thống trong lĩnh vực này.\n",
    "\n",
    "Chương này sẽ giúp chúng ta có cái nhìn tổng quan về các phương pháp NLP truyền thống, tạo nền tảng vững chắc trước khi chuyển sang các phương pháp hiện đại hơn.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Corpora, Tokens, and Types**📚💬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Corpus** 📚: Trong NLP, bất kỳ phương pháp nào, dù cổ điển hay hiện đại, đều bắt đầu từ một bộ dữ liệu văn bản, hay còn gọi là **corpus**. Một **corpus** thường chứa các **raw text** ✍️ và bất kỳ **metadata** 📝 nào liên quan đến văn bản đó.  \n",
    "    - **Raw text**: Là chuỗi ký tự thô, nhưng thường sẽ hiệu quả hơn khi chia các chuỗi ký tự này thành những đơn vị nhỏ hơn, gọi là **tokens** 🔠.\n",
    "    - **Metadata**: Là bất kỳ thông tin bổ sung nào được gán vào đoạn văn bản, ví dụ như **labels** 🔖 (nhãn phân loại) hoặc **timestamps** ⏱️ (dấu thời gian).\n",
    "  \n",
    "- Trong học máy, đoạn văn bản kèm theo metadata của nó được gọi là một **instance** 📍 hoặc **data point**. **Corpus** chính là một tập hợp các **instances**, được gọi chung là **dataset** 📊.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/chapter_02_figure_01.png\" alt=\"image\" />\n",
    "</div>\n",
    "\n",
    "- **Tokenization** 🔄: Là quá trình phân tách đoạn văn bản thành các **tokens** (đơn vị ngôn ngữ nhỏ nhất có ý nghĩa trong phân tích).\n",
    "  \n",
    "- **Types** 🔠: Là các token duy nhất trong **corpus**.  \n",
    "    - **Set of all Types**: Là **Vocabulary** 📖 hoặc **Corpus** 📚.\n",
    "    - Các từ có thể được chia thành hai loại chính: **content words** 📝 (từ mang nghĩa chính) và **stopwords** 🚫 (từ dừng).  \n",
    "    - **Stopwords**: Bao gồm những từ như mạo từ và giới từ, chủ yếu phục vụ mục đích ngữ pháp thay vì mang ý nghĩa cụ thể."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Mary, don't slap the green witch\"\n",
    "print([str(token) for token in nlp(text.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Snow', 'White', 'and', 'the', 'Seven', 'Degrees', '#MakeAMovieCold', '@midnight', ':-)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet = \"\"\"\n",
    "    Snow White and the Seven Degrees\n",
    "    #MakeAMovieCold@midnight:-) \n",
    "\"\"\"\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Unigrams, Bigrams, Trigrams, ..., N-grams** 🔠📊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong xử lý ngôn ngữ tự nhiên (NLP), **n-grams** là một khái niệm quan trọng dùng để mô tả chuỗi các đơn vị liên tiếp trong văn bản. N-grams có thể là các từ, ký tự, hoặc các đơn vị ngôn ngữ khác, giúp mô hình nắm bắt được cấu trúc và ngữ nghĩa của văn bản.\n",
    "\n",
    "- **Unigrams** 🅾️: Là các đơn vị riêng lẻ (từ đơn) trong văn bản. \n",
    "    - Ví dụ, trong câu `\"Học NLP thật thú vị\"`, các unigrams sẽ là: `[\"Học\", \"NLP\", \"thật\", \"thú\", \"vị\"]`.\n",
    "  \n",
    "- **Bigrams** 🔁: Là các cặp từ liên tiếp. \n",
    "    - Ví dụ, trong câu `\"Học NLP thật thú vị\"`, các bigrams sẽ là: `[\"Học NLP\", \"NLP thật\", \"thật thú\", \"thú vị\"]`.\n",
    "  \n",
    "- **Trigrams** 🔺: Là các nhóm ba từ liên tiếp. \n",
    "    - Ví dụ, từ câu `\"Học NLP thật thú vị\"`, trigrams sẽ là: `[\"Học NLP thật\", \"NLP thật thú\", \"thật thú vị\"]`.\n",
    "  \n",
    "- **N-grams** 🧩: Là chuỗi các từ có độ dài **n** tùy ý. N-grams có thể bao gồm unigrams, bigrams, trigrams, và các tổ hợp có độ dài cao hơn tùy thuộc vào mục tiêu của mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', ',', \"n't\"], [',', \"n't\", 'slap'], [\"n't\", 'slap', 'green'], ['slap', 'green', 'witch'], ['green', 'witch', '.']]\n"
     ]
    }
   ],
   "source": [
    "def n_gram(text, n):\n",
    "    '''\n",
    "    takes tokens or text, returns a list of n-grams\n",
    "    '''\n",
    "    return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "cleaned = ['mary', ',', \"n't\", 'slap', 'green', 'witch', '.']\n",
    "print(n_gram(cleaned, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Lemmas and Stems**🔤🌿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong NLP, việc hiểu và xử lý các **lemmas** và **stems** là rất quan trọng khi phân tích ngôn ngữ. Cả hai khái niệm này giúp giảm thiểu sự phức tạp của văn bản bằng cách biến các từ phức tạp thành dạng chuẩn của chúng.\n",
    "\n",
    "- **Lemmas** ✨: Là dạng chuẩn của một từ, được tìm thấy trong từ điển. **Lemmatization** là quá trình biến đổi một từ về dạng gốc của nó, đảm bảo nghĩa của từ không bị thay đổi. Ví dụ, từ \"better\" sẽ được chuyển thành \"good\", hoặc \"running\" sẽ trở thành \"run\". Quá trình này yêu cầu hiểu rõ ngữ cảnh và cú pháp của câu. Một số trường hợp chuyển đổi từ thành **lemmas** có thể hiệu quả vì giúp giảm số lượng từ trong **vocabulary**, đồng thời giữ cho chiều của vector biểu diễn không quá cao.\n",
    "\n",
    "- **Stems** 🌱: Là dạng rút gọn của một từ, được tạo ra bằng cách loại bỏ các hậu tố và tiền tố. Là phương pháp lemmatization đơn giản hơn. Nó sử dụng các quy tắc thủ công để loại bỏ các hậu tố của từ, giúp rút gọn từ về một dạng chung gọi là stems. Tuy nhiên, **stemmer** có thể không tạo ra từ chuẩn như **lemmatizer**. Ví dụ, từ \"running\" có thể được rút gọn thành \"run\", nhưng một số từ như \"better\" có thể trở thành \"bett\". **Stemming** thường không quan tâm đến ngữ cảnh và có thể dẫn đến các kết quả không chính xác. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he -> he\n",
      "was -> be\n",
      "running -> run\n",
      "late -> late\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"he was running late\")\n",
    "for token in doc:\n",
    "    print(f\"{token} -> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Categorizing Sentences and Documents** 📄🔖\n",
    "\n",
    "Các tác vụ **categorizing** hay **classifying documents** là một trong những ứng dụng sớm nhất của NLP. Các biểu diễn như **TF** hoặc **TF-IDF** rất hiệu quả đối với các tác vụ phân loại các chuỗi văn bản dài, như tài liệu và nhiều câu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorizing Words: POS Tagging** 🏷️🔠\n",
    "\n",
    "**POS tagging** (Part-of-Speech tagging) là quá trình phân loại từ trong câu theo các loại từ ngữ pháp như danh từ, động từ, tính từ, trạng từ, v.v. Đây là một bước quan trọng trong việc hiểu ngữ nghĩa của văn bản và giúp hệ thống NLP xử lý văn bản hiệu quả hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marry -> PROPN\n",
      "slapped -> VERB\n",
      "the -> DET\n",
      "green -> PROPN\n",
      "witch -> PROPN\n",
      ". -> PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Marry slapped the green witch.\")\n",
    "for token in doc:\n",
    "    print(f\"{token} -> {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorizing Spans: Chunking and Named Entity Recognition** 🧑‍💻🔍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thông thường, chúng ta cần gán nhãn cho chuỗi liên tiếp các **tokens**, quá trình này gọi là **chunking** 🧩 hoặc **shallow parsing** 🔍. Mục tiêu của chunking là tạo ra các đơn vị ngữ nghĩa cao cấp hơn, được cấu thành từ các thành phần ngữ pháp cơ bản như danh từ, động từ, tính từ, v.v. Nếu không có đủ dữ liệu để huấn luyện mô hình **shallow parsing** 🔍, bạn có thể sử dụng các **biểu thức chính quy** (regular expressions) 📜 trên các nhãn **part-of-speech tags** để xấp xỉ quá trình phân tích này.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/chapter_02_figure_02.png\" alt=\"image\" />\n",
    "</div>\n",
    "\n",
    "Một trong những loại **span** hữu ích là **named entity** 🏷️. **Named entity** là một chuỗi ký tự đại diện cho các đối tượng thực tế, chẳng hạn như con người, địa điểm, tổ chức, tên thuốc, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marry: NP\n",
      "the green witch: NP\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Marry slapped the green witch.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"{chunk}: {chunk.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Structure of Sentences** 🏗️📜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong khi **shallow parsing** là tác vụ xác định các **cụm từ**, thì việc xác định mối liên hệ giữa các cụm từ được gọi là **parsing**. 📚\n",
    "\n",
    "Có hai cách phổ biến để biểu diễn mối quan hệ này:\n",
    "\n",
    "- **Constituent Parsing** 🏗️: Các thành phần liên kết với nhau theo cấu trúc phân cấp, giúp biểu diễn mối quan hệ giữa các cụm từ trong câu.\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/chapter_02_figure_03.png\" alt=\"image\" />\n",
    "</div>\n",
    "\n",
    "- **Dependency Parsing** 🔗: Một cách khác để biểu diễn các mối quan hệ là phân tích phụ thuộc, trong đó mỗi từ phụ thuộc vào từ khác trong câu.\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/chapter_02_figure_04.png\" alt=\"image\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Word Senses and Semantic** 🧠🔤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một từ có thể có nhiều hơn một nghĩa, và những nghĩa khác nhau của một từ được gọi là **senses** 🧠. Từ ngữ thường mang nhiều **ngữ nghĩa** và sự khác biệt giữa các nghĩa này giúp xác định cách sử dụng của từ trong các ngữ cảnh khác nhau. **WordNet** 📖, một dự án tài nguyên từ vựng nổi tiếng của Đại học Princeton, mục tiêu lập danh mục các ngữ nghĩa của hầu hết các từ trong tiếng Anh, cùng với các mối quan hệ từ vựng khác giữa chúng.\n",
    "\n",
    "**Ví dụ**, từ \"plane\" có thể có nhiều nghĩa khác nhau trong các ngữ cảnh khác nhau.\n",
    "\n",
    "Những thập kỷ nghiên cứu và nỗ lực trong các dự án như **WordNet** rất đáng để bạn tận dụng, ngay cả khi hiện nay có sự xuất hiện của các phương pháp hiện đại 🌐.\n",
    "\n",
    "Ngoài ra, các **ngữ nghĩa** của từ cũng có thể được suy ra từ **ngữ cảnh** 📝. Việc tự động khám phá các ngữ nghĩa từ văn bản thực tế là nơi học bán giám sát (**semi-supervised learning**) lần đầu tiên được ứng dụng trong NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/chapter_02_figure_05.png\" alt=\"image\" />\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factorial_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
